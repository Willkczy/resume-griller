Metadata-Version: 2.4
Name: resume-griller
Version: 1.0.0
Summary: AI-powered interview simulator that grills candidates with resume-specific questions
Author-email: Your Name <your.email@example.com>
License: MIT
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.0.0
Requires-Dist: pdfplumber>=0.10.0
Requires-Dist: PyMuPDF>=1.23.0
Requires-Dist: langchain>=0.1.0
Requires-Dist: chromadb>=0.4.0
Requires-Dist: sentence-transformers>=2.2.0
Requires-Dist: fastapi>=0.109.0
Requires-Dist: uvicorn[standard]>=0.27.0
Requires-Dist: python-multipart>=0.0.6
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: pydantic>=2.5.0
Requires-Dist: pydantic-settings>=2.1.0
Requires-Dist: sqlalchemy>=2.0.0
Requires-Dist: aiosqlite>=0.19.0
Requires-Dist: httpx>=0.26.0
Requires-Dist: aiofiles>=23.2.0
Requires-Dist: anthropic>=0.18.0
Requires-Dist: openai>=1.12.0
Provides-Extra: ml
Requires-Dist: transformers>=4.36.0; extra == "ml"
Requires-Dist: peft>=0.7.0; extra == "ml"
Requires-Dist: accelerate>=0.25.0; extra == "ml"
Provides-Extra: dev
Requires-Dist: pytest>=7.4.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.23.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: black>=24.0.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Requires-Dist: mypy>=1.8.0; extra == "dev"
Requires-Dist: pre-commit>=3.6.0; extra == "dev"

# Interview Coach - AI-Powered Mock Interview System

An LLM-based application that generates contextual interview questions based on uploaded resumes. Built with LoRA fine-tuning and RAG pipeline.

##  Architecture
```
PDF Upload â†’ Resume Parser â†’ Chunker â†’ Embedder â†’ Retriever â†’ Fine-tuned LLM â†’ Questions
```

## ğŸ“ Project Structure
```
interview-coach/
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ resume_parser.py    # PDF/text parsing
â”‚   â”œâ”€â”€ chunker.py          # Semantic chunking
â”‚   â”œâ”€â”€ embedder.py         # Vector embeddings (ChromaDB)
â”‚   â”œâ”€â”€ retriever.py        # RAG retrieval
â”‚   â””â”€â”€ generator.py        # LLM question generation
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_resumes/     # Test resumes
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_parser.py
â”œâ”€â”€ export_prompts.py       # Export prompts for Colab inference
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

##  Quick Start

### Installation
```bash
conda create -n interview-coach python=3.11
conda activate interview-coach
pip install -r requirements.txt
```

### Run RAG Pipeline
```bash
# Test parser
python -m rag.resume_parser

# Test full pipeline
python -m rag.retriever

# Export prompts for Colab
python export_prompts.py
```

### Model Inference (Colab)

Model hosted on HuggingFace: [shubhampareek/interview-coach-lora](https://huggingface.co/shubhampareek/interview-coach-lora)

Use `Interview_Coach_Inference.ipynb` for GPU-accelerated inference.

##  Tech Stack

- **Fine-tuning:** LoRA on Mistral-7B
- **Embeddings:** sentence-transformers (all-MiniLM-L6-v2)
- **Vector DB:** ChromaDB
- **Framework:** PyTorch, Transformers, PEFT


##  License

MIT
