{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqvBkdYGS6-c"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install dependencies\n",
        "!pip install -q torch transformers peft accelerate\n",
        "\n",
        "# Cell 2: Load your fine-tuned model (no training needed)\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "LORA_MODEL = \"shubhampareek/interview-coach-lora\"  # Your HuggingFace model\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(LORA_MODEL)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model with LoRA\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, LORA_MODEL)\n",
        "model.eval()\n",
        "\n",
        "print(\"✅ Model loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define resume and prompt first\n",
        "test_resume = \"\"\"\n",
        "MICHAEL RODRIGUEZ\n",
        "DevOps Engineer & Cloud Architect\n",
        "\n",
        "michael.rodriguez@email.com\n",
        "Phone: (555) 456-7890\n",
        "Location: Austin, TX\n",
        "\n",
        "-----------------------------------------------------------\n",
        "CERTIFICATIONS\n",
        "-----------------------------------------------------------\n",
        "- AWS Solutions Architect Professional (2023)\n",
        "- Certified Kubernetes Administrator (CKA) (2022)\n",
        "- HashiCorp Terraform Associate (2022)\n",
        "- AWS DevOps Engineer Professional (2021)\n",
        "\n",
        "-----------------------------------------------------------\n",
        "TECHNICAL SKILLS\n",
        "-----------------------------------------------------------\n",
        "Cloud Platforms:     AWS, GCP, Azure\n",
        "Containerization:    Docker, Kubernetes, ECS, EKS\n",
        "IaC:                 Terraform, CloudFormation, Pulumi\n",
        "CI/CD:               Jenkins, GitLab CI, GitHub Actions, ArgoCD\n",
        "Monitoring:          Prometheus, Grafana, DataDog, ELK Stack\n",
        "Scripting:           Python, Bash, Go\n",
        "Version Control:     Git, GitHub, GitLab\n",
        "\n",
        "-----------------------------------------------------------\n",
        "WORK EXPERIENCE\n",
        "-----------------------------------------------------------\n",
        "Lead DevOps Engineer\n",
        "Amazon Web Services | 2021 - Present\n",
        "\n",
        "- Architected multi-region infrastructure supporting 99.99% uptime\n",
        "- Implemented GitOps workflow reducing deployment failures by 70%\n",
        "- Managed infrastructure costs, achieving $2M annual savings\n",
        "- Led migration of 200+ microservices to Kubernetes\n",
        "\n",
        "DevOps Engineer\n",
        "Stripe | 2018 - 2021\n",
        "\n",
        "- Built automated testing pipeline with 90% code coverage\n",
        "- Designed disaster recovery system with 15-minute RTO\n",
        "- Created self-service deployment platform for 50+ developers\n",
        "\n",
        "-----------------------------------------------------------\n",
        "EDUCATION\n",
        "-----------------------------------------------------------\n",
        "B.S. Computer Engineering\n",
        "Texas A&M University, 2018\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"Here is the candidate's resume:\n",
        "\n",
        "{test_resume}\n",
        "\n",
        "Generate 5 specific interview questions based on this resume.\"\"\"\n",
        "\n",
        "# Now generate\n",
        "response = generate_questions(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "B5WISYsUTDJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# More creative/varied questions\n",
        "response = generate_questions(prompt, max_tokens=500)\n",
        "\n",
        "# Or modify the function call with different temperature\n",
        "def generate_questions_custom(prompt, temperature=0.9, max_tokens=400):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert interviewer. Generate specific, insightful interview questions.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=temperature,  # Higher = more creative\n",
        "            do_sample=True,\n",
        "            top_p=0.95,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "# Test\n",
        "response = generate_questions_custom(prompt, temperature=0.9)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "LPcucqU8TIiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Upload and process exported prompts\n",
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "uploaded = files.upload()  # Upload exported_prompts.json\n",
        "\n",
        "with open(\"exported_prompts.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Cell 5: Generate questions\n",
        "results = []\n",
        "for item in data[\"prompts\"][:3]:  # Test with first 3\n",
        "    print(f\"Generating: {item['id']}\")\n",
        "    response = generate_questions(item[\"prompt\"])\n",
        "    results.append({\"id\": item[\"id\"], \"questions\": response})\n",
        "    print(response[:300] + \"...\\n\")\n",
        "\n",
        "print(\"✅ Done!\")"
      ],
      "metadata": {
        "id": "gTnGZvO7TK8D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}